{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64991489",
   "metadata": {},
   "source": [
    "<h1><b>Towards Object-Based Visual SLAM: A Revolution for Urban Tram</b></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eb87c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "import ast\n",
    "import cv2 \n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from tabulate import tabulate\n",
    "from math import ceil\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cfa786",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b96b35",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7f9a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Declare function to convert JSON annotations to a DataFrame\n",
    "\n",
    "def json_to_dataframe(annot_path, dataset_name='unknown'):\n",
    "    ds = dataset_name.lower()\n",
    "    records = []\n",
    "\n",
    "    for fname in os.listdir(annot_path):\n",
    "        if not fname.endswith('.json'):\n",
    "            continue\n",
    "\n",
    "        path = os.path.join(annot_path, fname)\n",
    "        with open(path, 'r') as f:\n",
    "            ann = json.load(f)\n",
    "\n",
    "        base = os.path.splitext(fname)[0] \n",
    "        file_ext = '.jpg' if ds == 'bdd100k' else '.png'\n",
    "\n",
    "        if not base.endswith(file_ext):\n",
    "            file_name = base + file_ext\n",
    "        else:\n",
    "            file_name = base\n",
    "\n",
    "        for obj in ann.get('objects', []):\n",
    "            ext = obj.get('points', {}).get('exterior', [])\n",
    "            if len(ext) < 2:\n",
    "                continue \n",
    "            xmin, ymin = ext[0]\n",
    "            xmax, ymax = ext[1]\n",
    "\n",
    "            rec = {\n",
    "                'file': file_name,\n",
    "                'type': obj.get('classTitle', 'unknown'),\n",
    "                'bbox_xmin': xmin,\n",
    "                'bbox_ymin': ymin,\n",
    "                'bbox_xmax': xmax,\n",
    "                'bbox_ymax': ymax\n",
    "            }\n",
    "\n",
    "            if ds == 'bdd100k':\n",
    "                attr = next(\n",
    "                    (t for t in obj.get('tags', []) if t.get('name') == 'attributes'),\n",
    "                    {}\n",
    "                )\n",
    "                attr_value = attr.get('value', '{}')\n",
    "                try:\n",
    "                    # Replace single quotes with double quotes for keys and values\n",
    "                    attr_value = attr_value.replace(\"'\", '\"')\n",
    "                    # Parse the string as a Python literal (handles True/False)\n",
    "                    d = ast.literal_eval(attr_value)\n",
    "                    rec['occluded'] = d.get('occluded', None)\n",
    "                    rec['truncated'] = d.get('truncated', None)\n",
    "                except (ValueError, SyntaxError):\n",
    "                    rec['occluded'] = None\n",
    "                    rec['truncated'] = None\n",
    "            \n",
    "            elif ds == 'kitti':\n",
    "                rec['occluded'] = next(\n",
    "                    (t['value'] for t in obj.get('tags', []) if t.get('name') == 'occlusion state'),\n",
    "                    None\n",
    "                )\n",
    "                rec['observation_angle'] = next(\n",
    "                    (float(t['value']) for t in obj.get('tags', []) if t.get('name') == 'observation angle'),\n",
    "                    0.0\n",
    "                )\n",
    "                dimensions = next(\n",
    "                    (t['value'].split() for t in obj.get('tags', []) if t['name'] == 'dimensions'),\n",
    "                    ['0.0', '0.0', '0.0']\n",
    "                )\n",
    "                rec['dimensions_h'] = float(dimensions[0]) if len(dimensions) > 0 else 0.0\n",
    "                rec['dimensions_w'] = float(dimensions[1]) if len(dimensions) > 1 else 0.0\n",
    "                rec['dimensions_l'] = float(dimensions[2]) if len(dimensions) > 2 else 0.0\n",
    "                location = next(\n",
    "                    (t['value'].split() for t in obj.get('tags', []) if t['name'] == 'location'),\n",
    "                    ['0.0', '0.0', '0.0']\n",
    "                )\n",
    "                rec['location_x'] = float(location[0]) if len(location) > 0 else 0.0\n",
    "                rec['location_y'] = float(location[1]) if len(location) > 1 else 0.0\n",
    "                rec['location_z'] = float(location[2]) if len(location) > 2 else 0.0\n",
    "                rec['rotation_y'] = next((float(t['value']) for t in obj.get('tags', []) if t['name'] == 'rotation y'), 0.0)\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(f'Unsupported dataset: {dataset_name}')\n",
    "\n",
    "            records.append(rec)\n",
    "\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5792ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare function to visualise box plots for bounding box area distributions of two BDD100K datasets side by side\n",
    "\n",
    "def plot_bbox_area_boxplot(\n",
    "    bdd100k1,\n",
    "    bdd100k2,\n",
    "    column='bbox_area',\n",
    "    label1='BDD100K Test',\n",
    "    label2='BDD100K Val',\n",
    "    figsize=(12, 5),\n",
    "    title=None\n",
    "):\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize, sharey=True)\n",
    "\n",
    "    # Left panel: BDD100K Test\n",
    "    axes[0].boxplot(bdd100k1[column].dropna(), patch_artist=True)\n",
    "    axes[0].set_title('')  # Remove title from top\n",
    "    axes[0].set_xlabel(label1)  # Move label to x-axis\n",
    "    axes[0].set_ylabel('Area (pixelsÂ²)')\n",
    "    axes[0].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Right panel: BDD100K Val\n",
    "    axes[1].boxplot(bdd100k2[column].dropna(), patch_artist=True)\n",
    "    axes[1].set_title('')  # Remove title from top\n",
    "    axes[1].set_xlabel(label2)  # Move label to x-axis\n",
    "    axes[1].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Overall title\n",
    "    fig.suptitle(title if title else f'Bounding Box Area Distribution: {label1} vs {label2}', fontsize=14, y=1.03)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea072358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot aspect ratio histograms side by side\n",
    "\n",
    "def plot_aspect_ratio_distributions(\n",
    "    bdd100k1,\n",
    "    bdd100k2,\n",
    "    label1='BDD100K Test',\n",
    "    label2='BDD100K Val',\n",
    "    bins=np.linspace(0, 5, 50),\n",
    "    title=None\n",
    "):\n",
    "    # Compute aspect ratios\n",
    "    bdd100k1['aspect_ratio'] = (\n",
    "        (bdd100k1['bbox_xmax'] - bdd100k1['bbox_xmin']) /\n",
    "        (bdd100k1['bbox_ymax'] - bdd100k1['bbox_ymin'])\n",
    "    )\n",
    "    bdd100k2['aspect_ratio'] = (\n",
    "        (bdd100k2['bbox_xmax'] - bdd100k2['bbox_xmin']) /\n",
    "        (bdd100k2['bbox_ymax'] - bdd100k2['bbox_ymin'])\n",
    "    )\n",
    "\n",
    "    # Determine y-axis limit\n",
    "    max1 = np.histogram(bdd100k1['aspect_ratio'], bins=bins)[0].max()\n",
    "    max2 = np.histogram(bdd100k2['aspect_ratio'], bins=bins)[0].max()\n",
    "    ymax = max(max1, max2)\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n",
    "\n",
    "    # Left panel: BDD100K Test\n",
    "    axes[0].hist(bdd100k1['aspect_ratio'], bins=bins, edgecolor='black', alpha=0.7)\n",
    "    axes[0].set_title(f'{label1} Aspect Ratio')\n",
    "    axes[0].set_xlabel('Aspect Ratio')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_ylim(0, ymax * 1.05)\n",
    "    axes[0].grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Right panel: BDD100K Val\n",
    "    axes[1].hist(bdd100k2['aspect_ratio'], bins=bins, edgecolor='black', alpha=0.7, color='C1')\n",
    "    axes[1].set_title(f'{label2} Aspect Ratio')\n",
    "    axes[1].set_xlabel('Aspect Ratio')\n",
    "    axes[1].set_ylim(0, ymax * 1.05)\n",
    "    axes[1].grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Overall title\n",
    "    fig.suptitle(title if title else f'Aspect Ratio Distributions: {label1} vs {label2}', fontsize=14, y=1.03)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fcd844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for logical errors in bounding box coordinates\n",
    "\n",
    "def check_logical_box_errors(df):\n",
    "    # Define image boundaries based on BDD100K resolution (1280x720)\n",
    "    img_width, img_height = 1280, 720\n",
    "    \n",
    "    # Check for logical errors (xmin > xmax or ymin > ymax)\n",
    "    invalid_logic = df[\n",
    "        (df['bbox_xmin'] > df['bbox_xmax']) |\n",
    "        (df['bbox_ymin'] > df['bbox_ymax'])\n",
    "    ]\n",
    "    \n",
    "    # Check for coordinates outside image boundaries\n",
    "    invalid_bounds = df[\n",
    "        (df['bbox_xmin'] < 0) | (df['bbox_xmin'] > img_width) |\n",
    "        (df['bbox_xmax'] < 0) | (df['bbox_xmax'] > img_width) |\n",
    "        (df['bbox_ymin'] < 0) | (df['bbox_ymin'] > img_height) |\n",
    "        (df['bbox_ymax'] < 0) | (df['bbox_ymax'] > img_height)\n",
    "    ]\n",
    "    \n",
    "    # Display logical errors\n",
    "    if not invalid_logic.empty:\n",
    "        print(f'Found {len(invalid_logic)} invalid bounding boxes with logical errors (xmin > xmax or ymin > ymax):')\n",
    "        print(tabulate(invalid_logic[['file', 'type', 'bbox_xmin', 'bbox_ymin', 'bbox_xmax', 'bbox_ymax']], \n",
    "                       headers='keys', tablefmt='pretty'))\n",
    "    else:\n",
    "        print('No logical errors found (xmin > xmax or ymin > ymax).')\n",
    "    \n",
    "    # Display boundary errors\n",
    "    if not invalid_bounds.empty:\n",
    "        print(f'Found {len(invalid_bounds)} invalid bounding boxes with boundary errors (outside 0-1279x0-719):')\n",
    "        print(tabulate(invalid_bounds[['file', 'type', 'bbox_xmin', 'bbox_ymin', 'bbox_xmax', 'bbox_ymax']], \n",
    "                       headers='keys', tablefmt='pretty'))\n",
    "    else:\n",
    "        print('No boundary errors found (outside 0-1279x0-719).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f12905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to correct invalid bounding boxes\n",
    "\n",
    "def correct_bounding_box(row, img_width=1280, img_height=720):\n",
    "    # Correct logical errors by swapping coordinates\n",
    "    if row['bbox_xmin'] > row['bbox_xmax']:\n",
    "        row['bbox_xmin'], row['bbox_xmax'] = row['bbox_xmax'], row['bbox_xmin']\n",
    "    if row['bbox_ymin'] > row['bbox_ymax']:\n",
    "        row['bbox_ymin'], row['bbox_ymax'] = row['bbox_ymax'], row['bbox_ymin']\n",
    "    \n",
    "    # Clamp coordinates only if they are out of bounds\n",
    "    # if row['bbox_xmin'] < 0 or row['bbox_xmin'] > img_width:\n",
    "    #     row['bbox_xmin'] = max(0, min(row['bbox_xmin'], img_width))\n",
    "    # if row['bbox_xmax'] < 0 or row['bbox_xmax'] > img_width:\n",
    "    #     row['bbox_xmax'] = max(0, min(row['bbox_xmax'], img_width))\n",
    "    # if row['bbox_ymin'] < 0 or row['bbox_ymin'] > img_height:\n",
    "    #     row['bbox_ymin'] = max(0, min(row['bbox_ymin'], img_height))\n",
    "    # if row['bbox_ymax'] < 0 or row['bbox_ymax'] > img_height:\n",
    "    #     row['bbox_ymax'] = max(0, min(row['bbox_ymax'], img_height))\n",
    "    \n",
    "    # Warn about zero-area boxes\n",
    "    if row['bbox_xmin'] == row['bbox_xmax'] or row['bbox_ymin'] == row['bbox_ymax']:\n",
    "        print(f\"Warning: Bounding box for {row['file']} has zero area after correction.\")\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5d6ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_bounding_box_corrections(df, img_width=1280, img_height=720):\n",
    "    # Check for logical errors (xmin > xmax or ymin > ymax)\n",
    "    invalid_logic = df[\n",
    "        (df['bbox_xmin'] > df['bbox_xmax']) |\n",
    "        (df['bbox_ymin'] > df['bbox_ymax'])\n",
    "    ]\n",
    "    \n",
    "    # Check for coordinates outside image boundaries\n",
    "    invalid_bounds = df[\n",
    "        (df['bbox_xmin'] < 0) | (df['bbox_xmin'] > img_width) |\n",
    "        (df['bbox_xmax'] < 0) | (df['bbox_xmax'] > img_width) |\n",
    "        (df['bbox_ymin'] < 0) | (df['bbox_ymin'] > img_height) |\n",
    "        (df['bbox_ymax'] < 0) | (df['bbox_ymax'] > img_height)\n",
    "    ]\n",
    "    \n",
    "    # Combine and remove duplicates if needed (though separate reporting is clearer)\n",
    "    total_invalid = len(invalid_logic) + len(invalid_bounds) - len(invalid_logic.merge(invalid_bounds, how='inner'))\n",
    "    \n",
    "    # Report logical errors\n",
    "    if not invalid_logic.empty:\n",
    "        print(f'Found {len(invalid_logic)} bounding boxes with remaining logical errors (xmin > xmax or ymin > ymax):')\n",
    "        print(tabulate(invalid_logic[['file', 'type', 'bbox_xmin', 'bbox_ymin', 'bbox_xmax', 'bbox_ymax']], \n",
    "                       headers='keys', tablefmt='pretty'))\n",
    "    else:\n",
    "        print('No logical errors found (xmin > xmax or ymin > ymax).')\n",
    "    \n",
    "    # Report boundary errors\n",
    "    if not invalid_bounds.empty:\n",
    "        print(f'Found {len(invalid_bounds)} bounding boxes with boundary errors (outside 0-1279x0-719):')\n",
    "        print(tabulate(invalid_bounds[['file', 'type', 'bbox_xmin', 'bbox_ymin', 'bbox_xmax', 'bbox_ymax']], \n",
    "                       headers='keys', tablefmt='pretty'))\n",
    "    else:\n",
    "        print('No boundary errors found (outside 0-1279x0-719).')\n",
    "    \n",
    "    # Summary\n",
    "    if total_invalid == 0:\n",
    "        print(f'All {len(df)} bounding boxes are valid after correction.')\n",
    "    else:\n",
    "        print(f'Total invalid bounding boxes after correction: {total_invalid}')\n",
    "\n",
    "# Example usage after applying corrections\n",
    "# bdd100k_df_cleaned = bdd100k_df_cleaned.apply(correct_bounding_box, axis=1)\n",
    "# verify_bounding_box_corrections(bdd100k_df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466bb4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate bounding box areas (width, height, and area)\n",
    "def calculate_box_areas(df):\n",
    "    df['bbox_width'] = df['bbox_xmax'] - df['bbox_xmin']\n",
    "    df['bbox_height'] = df['bbox_ymax'] - df['bbox_ymin']\n",
    "    df['bbox_area'] = df['bbox_width'] * df['bbox_height']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c13c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and remove bounding boxes with area less than a threshold\n",
    "\n",
    "def remove_noisy_boxes(df, min_area_threshold=100):\n",
    "    noisy_boxes = df[df['bbox_area'] < min_area_threshold]\n",
    "    if not noisy_boxes.empty:\n",
    "        print(f'Found {len(noisy_boxes)} noisy bounding boxes with area < {min_area_threshold} pixels:\\n')\n",
    "        preview = noisy_boxes[['file', 'type', 'bbox_xmin', 'bbox_ymin', 'bbox_xmax', 'bbox_ymax', 'bbox_area']].head(10)\n",
    "        print(tabulate(preview, headers='keys', tablefmt='fancy_grid', showindex=True))\n",
    "        df = df[df['bbox_area'] >= min_area_threshold]\n",
    "        print(f'\\nRemoved {len(noisy_boxes):,} noisy bounding boxes.')\n",
    "    else:\n",
    "        print(f'No bounding boxes found with area < {min_area_threshold} pixels.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9774132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the distribution of occlusion and truncation in the dataset\n",
    "\n",
    "def analyze_occlusion_truncation(df):\n",
    "    occlusion_counts = df['occluded'].value_counts()\n",
    "    truncation_counts = df['truncated'].value_counts()\n",
    "    \n",
    "    occlusion_table = [(k, v) for k, v in occlusion_counts.items()]\n",
    "    truncation_table = [(k, v) for k, v in truncation_counts.items()]\n",
    "    \n",
    "    print('Occlusion Distribution:')\n",
    "    print(tabulate(occlusion_table, headers=['Occluded', 'Count'], tablefmt='grid'))\n",
    "    \n",
    "    print('\\nTruncation Distribution:')\n",
    "    print(tabulate(truncation_table, headers=['Truncated', 'Count'], tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c176fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categories for occlusion and truncation\n",
    "\n",
    "occlusion_categories = {None, False, True}\n",
    "truncation_categories = {None, False, True}\n",
    "\n",
    "# Function to classify objects based on occlusion and truncation\n",
    "\n",
    "def classify_object(row):\n",
    "    occluded = row['occluded']\n",
    "    truncated = row['truncated']\n",
    "    \n",
    "    # Check if values are within expected set\n",
    "    if occluded not in occlusion_categories or truncated not in truncation_categories:\n",
    "        return 'unknown'\n",
    "    \n",
    "    # Handle None cases\n",
    "    if occluded is None or truncated is None:\n",
    "        return 'unknown'\n",
    "    \n",
    "    # Handle reliable and unreliable cases\n",
    "    if occluded is False and truncated is False:\n",
    "        return 'reliable'\n",
    "    if occluded is True or truncated is True:\n",
    "        return 'unreliable'\n",
    "    \n",
    "    return 'unknown' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe201d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude bounding boxes marked as unreliable\n",
    "\n",
    "def remove_unreliable_boxes(df):\n",
    "    unreliable_boxes = df[df['reliability'] == 'unreliable']\n",
    "    if not unreliable_boxes.empty:\n",
    "        print(f'\\nFound {len(unreliable_boxes)} unreliable bounding boxes.')\n",
    "        print(f'Length before removal: {len(df)}')\n",
    "        df = df[df['reliability'] != 'unreliable']\n",
    "        print(f'Length after removal: {len(df)}')\n",
    "        print(f'Removed {len(unreliable_boxes)} unreliable bounding boxes.')\n",
    "    else:\n",
    "        print('\\nNo unreliable bounding boxes found to remove.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7fadac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers in bounding box data based on area and aspect ratio\n",
    "\n",
    "def detect_outliers(df, check_area=True, check_aspect_ratio=True, area_multiplier=1.5, aspect_ratio_lower=0.2, aspect_ratio_upper=5):\n",
    "    if check_area and 'bbox_area' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must have 'bbox_area' column if check_area is True\")\n",
    "    if check_aspect_ratio and 'aspect_ratio' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must have 'aspect_ratio' column if check_aspect_ratio is True\")\n",
    "\n",
    "    if check_area:\n",
    "        Q1_area = df['bbox_area'].quantile(0.25)\n",
    "        Q3_area = df['bbox_area'].quantile(0.75)\n",
    "        IQR_area = Q3_area - Q1_area\n",
    "        lower_bound_area = Q1_area - area_multiplier * IQR_area\n",
    "        upper_bound_area = Q3_area + area_multiplier * IQR_area\n",
    "        area_outliers = (df['bbox_area'] < lower_bound_area) | (df['bbox_area'] > upper_bound_area)\n",
    "    else:\n",
    "        area_outliers = pd.Series([False] * len(df), index=df.index)\n",
    "\n",
    "    if check_aspect_ratio:\n",
    "        aspect_ratio_outliers = (df['aspect_ratio'] < aspect_ratio_lower) | (df['aspect_ratio'] > aspect_ratio_upper)\n",
    "    else:\n",
    "        aspect_ratio_outliers = pd.Series([False] * len(df), index=df.index)\n",
    "\n",
    "    outliers = area_outliers | aspect_ratio_outliers\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1820a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle aspect ratio outliers in BDD100K annotations by removing bounding boxes with extreme aspect ratios based on the IQR method per category\n",
    "\n",
    "def handle_aspect_ratio_outliers(df, category_column='type'):\n",
    "    # Step 1: Calculate aspect ratios\n",
    "    df = df.copy()  # Avoid modifying the original DataFrame\n",
    "    df['bbox_width'] = df['bbox_xmax'] - df['bbox_xmin']\n",
    "    df['bbox_height'] = df['bbox_ymax'] - df['bbox_ymin']\n",
    "    df = df[df['bbox_height'] > 0]  # Filter out invalid boxes (height <= 0)\n",
    "    df['aspect_ratio'] = df['bbox_width'] / df['bbox_height']\n",
    "\n",
    "    # Step 2: Compute IQR-based bounds per category\n",
    "    bounds = {}\n",
    "    for category in df[category_column].unique():\n",
    "        category_data = df[df[category_column] == category]['aspect_ratio'].dropna()\n",
    "        if len(category_data) > 0:\n",
    "            q1 = np.percentile(category_data, 25)\n",
    "            q3 = np.percentile(category_data, 75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            bounds[category] = (lower_bound, upper_bound)\n",
    "        else:\n",
    "            bounds[category] = (None, None)\n",
    "\n",
    "    # Step 3: Filter out outliers\n",
    "    def is_outlier(row):\n",
    "        category = row[category_column]\n",
    "        ar = row['aspect_ratio']\n",
    "        lower, upper = bounds[category]\n",
    "        if lower is not None and upper is not None:\n",
    "            return not (lower <= ar <= upper)\n",
    "        return False\n",
    "\n",
    "    df['is_outlier'] = df.apply(is_outlier, axis=1)\n",
    "    df_cleaned = df[~df['is_outlier']].drop(columns=['is_outlier', 'aspect_ratio', 'bbox_width', 'bbox_height'])\n",
    "\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e90c9b",
   "metadata": {},
   "source": [
    "# <b>1. EDA (EXPLORATORY DATA ANALYSIS)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed0c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "KITTI_ANNOT_PATH = 'KITTI/train/ann'\n",
    "KITTI_IMG_PATH = 'KITTI/train/img'\n",
    "BDD100K_ANNOT_PATH = 'BDD100K/train/ann'\n",
    "BDD100K_IMG_PATH = 'BDD100K/train/img'\n",
    "BDD100K_ANNOT_PATH_VAL = 'BDD100K/val/ann'\n",
    "BDD100K_IMG_PATH_VAL = 'BDD100K/val/img'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd54ebd",
   "metadata": {},
   "source": [
    "<h5>1.1 KITTI Object Detection Benchmark</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb040806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert KITTI JSON annotations to DataFrame\n",
    "\n",
    "kitti_df = json_to_dataframe(KITTI_ANNOT_PATH, dataset_name='KITTI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5019a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "kitti_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc78c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "kitti_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deb2e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_stat_kitti = kitti_df[['bbox_xmin', 'bbox_ymin', 'bbox_xmax', 'bbox_ymax']].describe().round(2)\n",
    "\n",
    "print('\\nSummary Statistics for Bounding Box Dimensions (KITTI):')\n",
    "print(tabulate(bbox_stat_kitti, headers='keys', tablefmt='pretty'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d335d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Chart: Distribution of Object Types\n",
    "\n",
    "counts = kitti_df['type'].value_counts()\n",
    "labels = counts.index.tolist()\n",
    "values = counts.values\n",
    "cmap = plt.get_cmap('tab10')\n",
    "colors = cmap.colors[:len(labels)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(labels, values, color=colors, edgecolor='black')\n",
    "# ax.set_title('Distribution of Object Types in KITTI')\n",
    "ax.set_xlabel('Classes')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3),  \n",
    "                textcoords='offset points',\n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ecc0c3",
   "metadata": {},
   "source": [
    "- `Car` dominates, while `truck`, `tram` and `person sitting` are minority classes.\n",
    "- This info provides insight that the prevalence of cars makes them ideal primary landmarks for SLAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21354806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Image with Annotations\n",
    "\n",
    "sample_file = kitti_df['file'].iloc[0]  \n",
    "print(f'Sample File: {sample_file}')\n",
    "sample_img_path = os.path.join(KITTI_IMG_PATH, sample_file)\n",
    "print(f'Sample Image Path: {sample_img_path}')\n",
    "if os.path.exists(sample_img_path):\n",
    "    img = cv2.imread(sample_img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  \n",
    "    annotations = kitti_df[kitti_df['file'] == sample_file]\n",
    "    for index, row in annotations.iterrows():\n",
    "        cv2.rectangle(img, (int(row['bbox_xmin']), int(row['bbox_ymin'])),\n",
    "                     (int(row['bbox_xmax']), int(row['bbox_ymax'])), (255, 0, 0), 2)\n",
    "        cv2.putText(img, row['type'], (int(row['bbox_xmin']), int(row['bbox_ymin'] - 10)),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(img)\n",
    "    # plt.title('Sample Image with Bounding Box Annotations')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f'Sample image {sample_img_path} not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5c475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file = kitti_df['file'].iloc[0]\n",
    "print(f'Sample File: {sample_file}')\n",
    "sample_img_path = os.path.join(KITTI_IMG_PATH, sample_file)\n",
    "print(f'Sample Image Path: {sample_img_path}')\n",
    "\n",
    "if os.path.exists(sample_img_path):\n",
    "    # Load original image\n",
    "    img_orig = cv2.imread(sample_img_path)\n",
    "    img_orig = cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Make a copy for annotations\n",
    "    img_annot = img_orig.copy()\n",
    "\n",
    "    # Filter annotations\n",
    "    annotations = kitti_df[kitti_df['file'] == sample_file]\n",
    "\n",
    "    # Draw bounding boxes and labels\n",
    "    for _, row in annotations.iterrows():\n",
    "        cv2.rectangle(\n",
    "            img_annot,\n",
    "            (int(row['bbox_xmin']), int(row['bbox_ymin'])),\n",
    "            (int(row['bbox_xmax']), int(row['bbox_ymax'])),\n",
    "            (255, 0, 0),\n",
    "            2\n",
    "        )\n",
    "        cv2.putText(\n",
    "            img_annot,\n",
    "            row['type'],\n",
    "            (int(row['bbox_xmin']), int(row['bbox_ymin']) - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            (255, 0, 0),\n",
    "            2\n",
    "        )\n",
    "\n",
    "    # Plot side by side\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    axs[0].imshow(img_orig)\n",
    "    axs[0].set_title('Original Image')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(img_annot)\n",
    "    axs[1].set_title('Annotated Image')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(f'Sample image {sample_img_path} not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae839ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display coordinate info for the first bounding box in the sample image\n",
    "\n",
    "sample_annotations = kitti_df[kitti_df['file'] == sample_file]\n",
    "if not sample_annotations.empty:\n",
    "    bbox_info = sample_annotations.iloc[0]\n",
    "    print(f'Bounding Box Coordinates for `{sample_file}`:')\n",
    "    print(f\"xmin: {bbox_info['bbox_xmin']}, ymin: {bbox_info['bbox_ymin']}, xmax: {bbox_info['bbox_xmax']}, ymax: {bbox_info['bbox_ymax']}\")\n",
    "else:\n",
    "    print(f'No bounding box found for {sample_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1689ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Image with Bounding Box Crop\n",
    "\n",
    "sample_file = kitti_df['file'].iloc[0]\n",
    "annotations = kitti_df[kitti_df['file'] == sample_file]\n",
    "\n",
    "sample_img_path = os.path.join(KITTI_IMG_PATH, sample_file)\n",
    "if annotations.empty or not os.path.exists(sample_img_path):\n",
    "    raise FileNotFoundError('No annotations or image for ' + sample_file)\n",
    "\n",
    "img = cv2.imread(sample_img_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "bbox = annotations.iloc[0]\n",
    "xmin, ymin = int(bbox['bbox_xmin']), int(bbox['bbox_ymin'])\n",
    "xmax, ymax = int(bbox['bbox_xmax']), int(bbox['bbox_ymax'])\n",
    "obj_img = img[ymin:ymax, xmin:xmax]\n",
    "h, w, _ = obj_img.shape \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "ax.imshow(obj_img, origin='upper')\n",
    "ax.axis('off')\n",
    "\n",
    "ax.add_patch(plt.Rectangle((0,0), w, h,\n",
    "                           linewidth=2,\n",
    "                           edgecolor='red',\n",
    "                           facecolor='none'))\n",
    "\n",
    "corners = [\n",
    "    (0,   0,   f'({xmin},{ymin})', 'left',  'top'),\n",
    "    (w,   0,   f'({xmax},{ymin})', 'right', 'top'),\n",
    "    (w,   h,   f'({xmax},{ymax})', 'right', 'bottom'),\n",
    "    (0,   h,   f'({xmin},{ymax})', 'left',  'bottom'),\n",
    "]\n",
    "for x, y, txt, ha, va in corners:\n",
    "    ax.text(x, y, txt,\n",
    "            color='yellow',\n",
    "            fontsize=10,\n",
    "            ha=ha, va=va,\n",
    "            bbox=dict(facecolor='black', alpha=0.5, pad=2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82633ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-Occurrence Matrix\n",
    "\n",
    "types_per_file = kitti_df.groupby('file')['type'].apply(list).to_dict()\n",
    "\n",
    "co_occurrence = defaultdict(set)\n",
    "for file, types in types_per_file.items():\n",
    "    for obj_type in types:\n",
    "        co_occurrence[obj_type].add(file)\n",
    "\n",
    "object_types = kitti_df['type'].unique()\n",
    "co_matrix = pd.DataFrame(0, index=object_types, columns=object_types, dtype=int)\n",
    "\n",
    "for i, type1 in enumerate(object_types):\n",
    "    for j, type2 in enumerate(object_types):\n",
    "        if i <= j:  \n",
    "            common_files = co_occurrence[type1] & co_occurrence[type2]\n",
    "            co_matrix.iloc[i, j] = len(common_files)\n",
    "            if i != j: \n",
    "                co_matrix.iloc[j, i] = len(common_files)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(co_matrix, cmap='viridis', interpolation='nearest')\n",
    "# plt.title('Co-Occurrence Matrix of Object Types in KITTI')\n",
    "plt.xticks(ticks=range(len(object_types)), labels=object_types, rotation=45)\n",
    "plt.yticks(ticks=range(len(object_types)), labels=object_types)\n",
    "plt.colorbar(label='Number of Co-Occurrences')\n",
    "for i in range(len(object_types)):\n",
    "    for j in range(len(object_types)):\n",
    "        count = co_matrix.iloc[i, j]\n",
    "        plt.text(j, i, co_matrix.iloc[i, j], ha='center', va='center', color='black' if count > 5000 else 'white')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a5fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial Heatmap for each object type\n",
    "\n",
    "image_width = 1224  \n",
    "image_height = 370  \n",
    "\n",
    "kitti_data_individual = []\n",
    "for json_file in os.listdir(KITTI_ANNOT_PATH):\n",
    "    if json_file.endswith('.json'):\n",
    "        json_file_path = os.path.join(KITTI_ANNOT_PATH, json_file)\n",
    "        with open(json_file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            base_name = os.path.splitext(json_file)[0]\n",
    "            file_name = f'{base_name}.png'\n",
    "            objects = data.get('objects', [])\n",
    "            for obj in objects:\n",
    "                record = {\n",
    "                    'file': file_name,\n",
    "                    'type': obj.get('classTitle', 'unknown'),\n",
    "                    'bbox_xmin': obj['points']['exterior'][0][0],\n",
    "                    'bbox_ymin': obj['points']['exterior'][0][1],\n",
    "                    'bbox_xmax': obj['points']['exterior'][1][0],\n",
    "                    'bbox_ymax': obj['points']['exterior'][1][1]\n",
    "                }\n",
    "                kitti_data_individual.append(record)\n",
    "\n",
    "kitti_df = pd.DataFrame(kitti_data_individual)\n",
    "\n",
    "image_width   = 1224\n",
    "image_height  = 370\n",
    "object_types  = kitti_df['type'].unique().tolist()\n",
    "n_types       = len(object_types)\n",
    "n_cols        = 3\n",
    "n_rows        = ceil(n_types / n_cols)\n",
    "\n",
    "all_heatmaps = []\n",
    "for t in object_types:\n",
    "    df_t = kitti_df[kitti_df['type'] == t]\n",
    "    x_centers = (df_t['bbox_xmin'] + df_t['bbox_xmax']) / 2\n",
    "    y_centers = (df_t['bbox_ymin'] + df_t['bbox_ymax']) / 2\n",
    "    heatmap, _, _ = np.histogram2d(\n",
    "        x_centers, y_centers,\n",
    "        bins=[50, 50],\n",
    "        range=[[0, image_width], [0, image_height]]\n",
    "    )\n",
    "    all_heatmaps.append(heatmap.T)  \n",
    "\n",
    "vmin = min(h.min() for h in all_heatmaps)\n",
    "vmax = max(h.max() for h in all_heatmaps)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    n_rows, n_cols,\n",
    "    figsize=(n_cols * 5, n_rows * 4),\n",
    "    sharex=False, sharey=False\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (ax, obj_type) in enumerate(zip(axes, object_types)):\n",
    "    hm = all_heatmaps[idx]\n",
    "    im = ax.imshow(\n",
    "        hm,\n",
    "        cmap='hot',\n",
    "        origin='lower',\n",
    "        extent=[0, image_width, 0, image_height],\n",
    "        aspect='auto',\n",
    "        vmin=vmin, vmax=vmax\n",
    "    )\n",
    "    ax.set_title(f'Spatial Heatmap for {obj_type}')\n",
    "    ax.set_xlabel('X Coordinate (pixels)')\n",
    "    ax.set_ylabel('Y Coordinate (pixels)')\n",
    "    \n",
    "    cbar = fig.colorbar(\n",
    "        im, ax=ax,\n",
    "        fraction=0.046,  \n",
    "        pad=0.04         \n",
    "    )\n",
    "    cbar.set_label('Density')\n",
    "\n",
    "for j in range(n_types, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "# fig.suptitle('Spatial Heatmaps of Object Classes (KITTI)', y=1.02, fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd87cc72",
   "metadata": {},
   "source": [
    "<h5>1.2 BDD100K</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a8738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert BDD100K JSON annotations to DataFrame (train vs val set)\n",
    "\n",
    "bdd100k_df = json_to_dataframe(BDD100K_ANNOT_PATH, dataset_name='BDD100K')\n",
    "bdd100k_df_val = json_to_dataframe(BDD100K_ANNOT_PATH_VAL, dataset_name='BDD100K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6593e8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd100k_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e22d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd100k_df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a183bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bdd100k_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f583e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bdd100k_df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f34e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd100k_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e6b4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd100k_df_val.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828528d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_stat_bdd100k = bdd100k_df[['bbox_xmin', 'bbox_ymin', 'bbox_xmax', 'bbox_ymax']].describe().round(2)\n",
    "\n",
    "print('\\nSummary Statistics for Bounding Box Dimensions (BDD100K):')\n",
    "print(tabulate(bbox_stat_bdd100k, headers='keys', tablefmt='pretty'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb85c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Chart: Distribution of Object Types\n",
    "\n",
    "counts = bdd100k_df['type'].value_counts()\n",
    "labels = counts.index.tolist()\n",
    "values = counts.values\n",
    "cmap = plt.get_cmap('tab10')\n",
    "colors = cmap.colors[:len(labels)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(labels, values, color=colors, edgecolor='black')\n",
    "# ax.set_title('Distribution of Object Types in BDD100K')\n",
    "ax.set_xlabel('Classes')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3),  \n",
    "                textcoords='offset points',\n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc9fea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Image with Annotations for BDD100K\n",
    "\n",
    "# sample_file = bdd100k_df['file'].iloc[0]\n",
    "sample_file = '0a006b7b-c22407a2.jpg'\n",
    "print(f'Sample File: {sample_file}')\n",
    "sample_img_path = os.path.join('BDD100K/train/img', sample_file)\n",
    "print(f'Sample Image Path: {sample_img_path}')\n",
    "\n",
    "if os.path.exists(sample_img_path):\n",
    "    img_orig = cv2.imread(sample_img_path)\n",
    "    img_orig = cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    img_annot = img_orig.copy()\n",
    "\n",
    "    annotations = bdd100k_df[bdd100k_df['file'] == sample_file]\n",
    "\n",
    "    for _, row in annotations.iterrows():\n",
    "        cv2.rectangle(\n",
    "            img_annot,\n",
    "            (int(row['bbox_xmin']), int(row['bbox_ymin'])),\n",
    "            (int(row['bbox_xmax']), int(row['bbox_ymax'])),\n",
    "            (255, 0, 0),\n",
    "            2\n",
    "        )\n",
    "        cv2.putText(\n",
    "            img_annot,\n",
    "            row['type'],\n",
    "            (int(row['bbox_xmin']), int(row['bbox_ymin']) - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            (255, 0, 0),\n",
    "            2\n",
    "        )\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    axs[0].imshow(img_orig)\n",
    "    axs[0].set_title('Original Image')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(img_annot)\n",
    "    axs[1].set_title('Annotated Image')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(f'Sample image {sample_img_path} not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af128412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cropped image with bounding box corner labels\n",
    "\n",
    "# sample_file = bdd100k_df['file'].iloc[0]\n",
    "annotations = bdd100k_df[bdd100k_df['file'] == sample_file]\n",
    "\n",
    "sample_img_path = os.path.join('BDD100K/train/img', sample_file)\n",
    "if annotations.empty or not os.path.exists(sample_img_path):\n",
    "    raise FileNotFoundError('No annotations or image for ' + sample_file)\n",
    "\n",
    "img = cv2.imread(sample_img_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "bbox = annotations.iloc[15]\n",
    "xmin, ymin = int(bbox['bbox_xmin']), int(bbox['bbox_ymin'])\n",
    "xmax, ymax = int(bbox['bbox_xmax']), int(bbox['bbox_ymax'])\n",
    "\n",
    "obj_img = img[ymin:ymax, xmin:xmax]\n",
    "h, w, _ = obj_img.shape\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "ax.imshow(obj_img, origin='upper')\n",
    "ax.axis('off')\n",
    "\n",
    "ax.add_patch(plt.Rectangle((0,0), w, h,\n",
    "                           linewidth=2,\n",
    "                           edgecolor='red',\n",
    "                           facecolor='none'))\n",
    "\n",
    "corners = [\n",
    "    (0,   0,   f'({xmin},{ymin})', 'left',  'top'),\n",
    "    (w,   0,   f'({xmax},{ymin})', 'right', 'top'),\n",
    "    (w,   h,   f'({xmax},{ymax})', 'right', 'bottom'),\n",
    "    (0,   h,   f'({xmin},{ymax})', 'left',  'bottom'),\n",
    "]\n",
    "for x, y, txt, ha, va in corners:\n",
    "    ax.text(\n",
    "        x, y, txt,\n",
    "        color='yellow',\n",
    "        fontsize=10,\n",
    "        ha=ha,\n",
    "        va=va,\n",
    "        bbox=dict(facecolor='black', alpha=0.5, pad=2)\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c8b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-Occurrence Matrix Heatmap\n",
    "\n",
    "types_per_file = bdd100k_df.groupby('file')['type'].apply(set).to_dict() \n",
    "co_occurrence = defaultdict(set)\n",
    "for file, types in types_per_file.items():\n",
    "    for obj_type in types:\n",
    "        co_occurrence[obj_type].add(file)\n",
    "\n",
    "# Use top 12 classes as per provided distribution\n",
    "# top_classes = ['car', 'lane', 'traffic sign', 'traffic light', 'drivable area', 'person', 'truck', 'bus', 'bike', 'rider', 'motor', 'train']\n",
    "top_classes = bdd100k_df['type'].unique()\n",
    "co_matrix = pd.DataFrame(0, index=top_classes, columns=top_classes, dtype=int)\n",
    "\n",
    "# Calculate unique images per class for diagonal\n",
    "unique_images_per_class = {cls: len(co_occurrence[cls]) for cls in top_classes}\n",
    "\n",
    "for i, type1 in enumerate(top_classes):\n",
    "    for j, type2 in enumerate(top_classes):\n",
    "        if i <= j:\n",
    "            if i == j:\n",
    "                # Diagonal: number of unique images with at least one instance of the class\n",
    "                co_matrix.iloc[i, j] = unique_images_per_class[type1]\n",
    "            else:\n",
    "                # Off-diagonal: number of images where both types co-occur (unique pair per image)\n",
    "                common_files = co_occurrence[type1] & co_occurrence[type2]\n",
    "                co_matrix.iloc[i, j] = len(common_files)\n",
    "                co_matrix.iloc[j, i] = len(common_files)\n",
    "\n",
    "total_images = bdd100k_df['file'].nunique()\n",
    "print('Total unique images:', total_images)\n",
    "print('Sample co-occurrence (car & person):', len(co_occurrence['car'] & co_occurrence['person']))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(co_matrix, cmap='viridis', interpolation='nearest', vmin=0, vmax=10000)\n",
    "# plt.title('Co-Occurrence Matrix of Object Types in BDD100K Train Set')\n",
    "plt.xticks(ticks=range(len(top_classes)), labels=top_classes, rotation=45)\n",
    "plt.yticks(ticks=range(len(top_classes)), labels=top_classes)\n",
    "plt.colorbar(label='Count')\n",
    "for i in range(len(top_classes)):\n",
    "    for j in range(len(top_classes)):\n",
    "        count = co_matrix.iloc[i, j]\n",
    "        plt.text(j, i, count if count > 0 else '', ha='center', va='center', color='black' if count > 5000 else 'white', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd10dc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial Heatmap for each object type in BDD100K\n",
    "\n",
    "image_width, image_height = 1280, 720 \n",
    "counts = bdd100k_df['type'].value_counts()\n",
    "top9 = counts.head(9).index.tolist()  \n",
    "n_types = len(top9)\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(n_types / n_cols))\n",
    "\n",
    "all_hms = []\n",
    "for cls in top9:\n",
    "    dfc = bdd100k_df[bdd100k_df['type'] == cls]\n",
    "    x_ctr = (dfc['bbox_xmin'] + dfc['bbox_xmax']) / 2\n",
    "    y_ctr = (dfc['bbox_ymin'] + dfc['bbox_ymax']) / 2\n",
    "\n",
    "    hm, _, _ = np.histogram2d(\n",
    "        x_ctr, y_ctr,\n",
    "        bins=[50, 50],\n",
    "        range=[[0, image_width], [0, image_height]]\n",
    "    )\n",
    "    all_hms.append(hm.T)\n",
    "\n",
    "vmin = min(h.min() for h in all_hms)\n",
    "vmax = max(h.max() for h in all_hms)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    n_rows, n_cols,\n",
    "    figsize=(n_cols * 5, n_rows * 4),\n",
    "    squeeze=False\n",
    ")\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for idx, cls in enumerate(top9):\n",
    "    ax = axes_flat[idx]\n",
    "    im = ax.imshow(\n",
    "        all_hms[idx],\n",
    "        cmap='hot',\n",
    "        origin='lower',\n",
    "        extent=[0, image_width, 0, image_height],\n",
    "        aspect='auto',\n",
    "        vmin=vmin, vmax=vmax\n",
    "    )\n",
    "    # Add titles for each heatmap\n",
    "    ax.set_title(f'Spatial Heatmap for {cls}')\n",
    "    ax.set_xlabel('X Coordinate (px)')\n",
    "    ax.set_ylabel('Y Coordinate (px)')\n",
    "\n",
    "    cbar = fig.colorbar(\n",
    "        im, ax=ax,\n",
    "        fraction=0.046, pad=0.04\n",
    "    )\n",
    "    cbar.set_label('Density')\n",
    "\n",
    "for j in range(n_types, len(axes_flat)):\n",
    "    axes_flat[j].axis('off')\n",
    "\n",
    "# fig.suptitle('Spatial Heatmaps of Object Classes (BDD100K)', y=1.02, fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f0a69c",
   "metadata": {},
   "source": [
    "# <b>2. DATA CLEANING</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c417df89",
   "metadata": {},
   "source": [
    "<h5>2.1 Validation of Bounding Box Coordinates</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7396afc2",
   "metadata": {},
   "source": [
    "<b>Train set</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea9089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_logical_box_errors(bdd100k_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f312ef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply correction for train set\n",
    "\n",
    "bdd100k_df_cleaned = bdd100k_df.copy()\n",
    "bdd100k_df_cleaned = bdd100k_df_cleaned.apply(correct_bounding_box, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35714566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify correction for train set\n",
    "\n",
    "verify_bounding_box_corrections(bdd100k_df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916352ef",
   "metadata": {},
   "source": [
    "<b>Val set</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4af8bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_logical_box_errors(bdd100k_df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply correction for val set\n",
    "\n",
    "bdd100k_df_val_cleaned = bdd100k_df_val.copy()\n",
    "bdd100k_df_val_cleaned = bdd100k_df_val_cleaned.apply(correct_bounding_box, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a94204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify correction for val set\n",
    "\n",
    "verify_bounding_box_corrections(bdd100k_df_val_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9cc9ba",
   "metadata": {},
   "source": [
    "<h5>2.2 Removal of Noisy Annotations</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81244726",
   "metadata": {},
   "source": [
    "<b>Train set</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1279bbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set minimum area threshold: 100 pixels (10x10 pixels)\n",
    "\n",
    "MIN_AREA_THRESHOLD = 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0b98e5",
   "metadata": {},
   "source": [
    "A 100-pixel threshold aligns with filtering out very small, potentially erroneous boxes given the 1280x720 resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4d6802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate bounding box areas\n",
    "\n",
    "bdd100k_df_cleaned = calculate_box_areas(bdd100k_df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97587d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and remove noisy annotations (boxes with area < threshold)\n",
    "\n",
    "bdd100k_df_cleaned = remove_noisy_boxes(df=bdd100k_df_cleaned, min_area_threshold=MIN_AREA_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edab4261",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bdd100k_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184d097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bdd100k_df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7552f35b",
   "metadata": {},
   "source": [
    "<b>Val set</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539df36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate bounding box areas\n",
    "\n",
    "bdd100k_df_val_cleaned = calculate_box_areas(bdd100k_df_val_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cba48d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd100k_df_val_cleaned = remove_noisy_boxes(df=bdd100k_df_val_cleaned, min_area_threshold=MIN_AREA_THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f75a6",
   "metadata": {},
   "source": [
    "<h5>2.3 Handling Occluded or Truncated Objects</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44312f43",
   "metadata": {},
   "source": [
    "<b>Test set</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fe448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse occlusion and truncation distribution\n",
    "\n",
    "analyze_occlusion_truncation(bdd100k_df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd89ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply classification\n",
    "\n",
    "bdd100k_df_cleaned = bdd100k_df_cleaned.copy()\n",
    "bdd100k_df_cleaned['reliability'] = bdd100k_df_cleaned.apply(classify_object, axis=1)\n",
    "reliability_counts = bdd100k_df_cleaned['reliability'].value_counts()\n",
    "print('\\nReliability Classification:')\n",
    "print(reliability_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1022459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude unreliable objects\n",
    "\n",
    "bdd100k_df_cleaned = remove_unreliable_boxes(bdd100k_df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbfc324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the updated DataFrame\n",
    "\n",
    "print(f'\\nFinal number of bounding boxes after handling occluded/truncated objects: {len(bdd100k_df_cleaned)}')\n",
    "print('Cleaned DataFrame head:')\n",
    "bdd100k_df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b2817",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd100k_df_cleaned['reliability'].value_counts().plot(kind='bar', color=['green', 'red'], alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372180f3",
   "metadata": {},
   "source": [
    "- <b>Relevance for SLAM:</b> Research suggests that keeping the `unknown` records in the dataset is likely beneficial for training YOLOv12, as they may include valid annotations for classes like Lane and Drivable Area, which could still be useful for object detection in visual SLAM.\n",
    "\n",
    "- <b>Class Balance</b>: With 600,000 `unknown` records, removing them could skew the dataset, especially if they include significant classes. Class weights in YOLOv12 training can mitigate imbalance if kept.\n",
    "\n",
    "&rarr; Keep the `unknown` records in the dataset for training YOLOv12 (ensuring a diverse and comprehensive dataset for predictive modelling in big data analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a61db9",
   "metadata": {},
   "source": [
    "<b>Val set</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88c31be",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_occlusion_truncation(bdd100k_df_val_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b8de12",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd100k_df_val_cleaned = bdd100k_df_val_cleaned.copy()\n",
    "bdd100k_df_val_cleaned['reliability'] = bdd100k_df_val_cleaned.apply(classify_object, axis=1)\n",
    "reliability_val_counts = bdd100k_df_val_cleaned['reliability'].value_counts()\n",
    "print('\\nReliability Classification:')\n",
    "print(reliability_val_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883e75b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd100k_df_val_cleaned = remove_unreliable_boxes(bdd100k_df_val_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceeff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the updated DataFrame val set\n",
    "\n",
    "print(f'\\nFinal number of bounding boxes (val set) after handling occluded/truncated objects: {len(bdd100k_df_val_cleaned)}')\n",
    "print('Cleaned DataFrame head:')\n",
    "bdd100k_df_val_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e963d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd100k_df_val_cleaned['reliability'].value_counts().plot(kind='bar', color=['green', 'red'], alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f156d3",
   "metadata": {},
   "source": [
    "<h5>2.4 Outlier Handling</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6895c9",
   "metadata": {},
   "source": [
    "<b>Statistical Outliers</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee1c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary statistics for bbox_area to understand the distribution\n",
    "bbox_area_stats = bdd100k_df_cleaned['bbox_area'].describe()\n",
    "print('\\nSummary Statistics for Bounding Box Area (test set):')\n",
    "print(tabulate(bbox_area_stats.to_frame().T, headers='keys', tablefmt='pretty'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6a2b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_area_stats_val = bdd100k_df_val_cleaned['bbox_area'].describe()\n",
    "print('\\nSummary Statistics for Bounding Box Area (val set):')\n",
    "print(tabulate(bbox_area_stats_val.to_frame().T, headers='keys', tablefmt='pretty'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3c0c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect and remove outliers (test set)\n",
    "    \n",
    "while True:\n",
    "    outliers = detect_outliers(bdd100k_df_cleaned, check_aspect_ratio=False)\n",
    "    num_outliers = outliers.sum()\n",
    "    \n",
    "    # Exit the loop if no outliers remain\n",
    "    if num_outliers == 0:\n",
    "        print('No more outliers found. Process complete.')\n",
    "        break\n",
    "    \n",
    "    print(f'Found {num_outliers} outliers.')\n",
    "    bdd100k_df_cleaned = bdd100k_df_cleaned[~outliers]\n",
    "    print(f'Removed {num_outliers} outliers.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4358586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect and remove outliers (val set)\n",
    "    \n",
    "while True:\n",
    "    outliers_val = detect_outliers(bdd100k_df_val_cleaned, check_aspect_ratio=False)\n",
    "    num_outliers_val = outliers_val.sum()\n",
    "    \n",
    "    # Exit the loop if no outliers remain\n",
    "    if num_outliers_val == 0:\n",
    "        print('No more outliers found. Process complete.')\n",
    "        break\n",
    "    \n",
    "    print(f'Found {num_outliers_val} outliers.')\n",
    "    bdd100k_df_val_cleaned = bdd100k_df_val_cleaned[~outliers_val]\n",
    "    print(f'Removed {num_outliers_val} outliers.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f1a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd100k_df_outlier_cleaned = bdd100k_df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f2ef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bdd100k_df_outlier_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2c57cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd100k_df_val_outlier_cleaned = bdd100k_df_val_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623c35be",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bdd100k_df_val_outlier_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5530ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bbox_area_boxplot(bdd100k_df_outlier_cleaned, bdd100k_df_val_outlier_cleaned, title='Bounding Box Area Distribution: BDD100K Test vs BDD100K Val - After Outlier Removal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5365066",
   "metadata": {},
   "source": [
    "<b>Aspect Ratio Outliers</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f14777",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd100k_df_cleaned_filtered = handle_aspect_ratio_outliers(bdd100k_df_outlier_cleaned)\n",
    "print(f'Original DataFrame size: {len(bdd100k_df_cleaned)}')\n",
    "print(f'Filtered DataFrame size: {len(bdd100k_df_cleaned_filtered)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea8108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd100k_df_val_cleaned_filtered = handle_aspect_ratio_outliers(bdd100k_df_val_outlier_cleaned)\n",
    "print(f'Original DataFrame size: {len(bdd100k_df_cleaned)}')\n",
    "print(f'Filtered DataFrame size: {len(bdd100k_df_val_cleaned_filtered)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fb284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the original DataFrame\n",
    "bdd100k_df_cleaned = bdd100k_df_cleaned_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749f1ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd100k_df_val_cleaned = bdd100k_df_val_cleaned_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d01825",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_aspect_ratio_distributions(bdd100k_df_cleaned_filtered, bdd100k_df_val_cleaned_filtered, title='Aspect Ratio Distributions: BDD100K Test vs BDD100K Val after aspect outlier removal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8be2e7b",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{aspect\\_ratio}\n",
    "\\;=\\;\n",
    "\\frac{\\mathrm{width}}{\\mathrm{height}}\n",
    "\\;=\\;\n",
    "\\frac{x_{\\max} \\;-\\; x_{\\min}}\n",
    "     {y_{\\max} \\;-\\; y_{\\min}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827a347b",
   "metadata": {},
   "source": [
    "- The histogram has not changed much because the outlier removal process affected only a small percentage (4.95%) of the dataset, and these outliers were distributed across the tails of the distribution rather than concentrated in a way that would reshape the histogram.\n",
    "\n",
    "- The peak and dense central region (0.5 to 2.0) remain intact, and the sparse tails were only slightly trimmed, which is not visually significant given the scale and binning of the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f013ee",
   "metadata": {},
   "source": [
    "# <b>3. DATA PRE-PROCESSING & MODELLING</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da74f9dc",
   "metadata": {},
   "source": [
    "<h5>3.1 Normalisation</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87964135",
   "metadata": {},
   "source": [
    "To stabilise training and account for potential differences in image intensity between BDD100K and KITTI, we need to compute the mean and standard deviation of RGB pixel values for the BDD100K dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc40787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd100k_df_cleaned['image_path'] = bdd100k_df_cleaned['file'].apply(lambda x: os.path.join(BDD100K_IMG_PATH, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf084068",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd100k_df_val_cleaned['image_path'] = bdd100k_df_val_cleaned['file'].apply(lambda x: os.path.join(BDD100K_IMG_PATH_VAL, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add31ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique image paths (since some images have multiple objects)\n",
    "image_paths = bdd100k_df_cleaned['image_path'].unique().tolist()\n",
    "image_paths_val = bdd100k_df_val_cleaned['image_path'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0a6dc2",
   "metadata": {},
   "source": [
    "<h5>3.2 Convert Annotations to YOLO Format</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea822c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class mapping for your object types\n",
    "\n",
    "class_mapping = {\n",
    "    'car': 0,\n",
    "    'lane': 1,\n",
    "    'traffic sign': 2,\n",
    "    'traffic light': 3,\n",
    "    'drivable area': 4,\n",
    "    'person': 5,\n",
    "    'truck': 6,\n",
    "    'bus': 7,\n",
    "    'bike': 8,\n",
    "    'rider': 9,\n",
    "    'motor': 10,\n",
    "    'train': 11\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f38a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame annotations to YOLO format and save as .txt files.\n",
    "\n",
    "def convert_to_yolo(df, output_dir, img_width=1280, img_height=720):\n",
    "    for file, group in df.groupby('file'):\n",
    "        txt_file = os.path.join(output_dir, os.path.splitext(file)[0] + '.txt')\n",
    "        os.makedirs(output_dir, exist_ok=True)  # Create directory if it doesnât exist\n",
    "        with open(txt_file, 'w') as f:\n",
    "            for _, row in group.iterrows():\n",
    "                class_id = class_mapping.get(row['type'], -1)\n",
    "                if class_id == -1:\n",
    "                    continue  # Skip unknown classes\n",
    "                xmin, ymin, xmax, ymax = row['bbox_xmin'], row['bbox_ymin'], row['bbox_xmax'], row['bbox_ymax']\n",
    "                x_center = (xmin + xmax) / 2 / img_width\n",
    "                y_center = (ymin + ymax) / 2 / img_height\n",
    "                width = (xmax - xmin) / img_width\n",
    "                height = (ymax - ymin) / img_height\n",
    "                f.write(f'{class_id} {x_center} {y_center} {width} {height}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be741d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting train/val labels to YOLO format\n",
    "convert_to_yolo(bdd100k_df_cleaned, 'BDD100K_TRAIN/labels/train')\n",
    "convert_to_yolo(bdd100k_df_val_cleaned, 'BDD100K_TRAIN/labels/val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca028473",
   "metadata": {},
   "source": [
    "<h5>3.3 Data Augmentation and YOLOv12 Training</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f2f9cd",
   "metadata": {},
   "source": [
    "- Apply data augmentations like random scaling, horizontal flipping, color jittering, and random cropping to enhance model robustness, especially to bridge the domain gap.\n",
    "- YOLOv12, developed by Ultralytics, automatically applies letterboxing when we specify the input size (imgsz=416) during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52356ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small, pretrained on COCO\n",
    "model = YOLO('yolo12s.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a27e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# results = model.train(\n",
    "#     data='bdd100k.yaml',\n",
    "#     epochs=2,\n",
    "#     imgsz=416,\n",
    "#     device='mps',\n",
    "#     batch=8,\n",
    "#     lr0=1e-3,\n",
    "#     workers=5,\n",
    "#     flipud=0.0,           # No vertical flip\n",
    "#     fliplr=0.5,           # Horizontal flip with 50% probability\n",
    "#     hsv_h=0.2,            # Hue jitter\n",
    "#     hsv_s=0.2,            # Saturation jitter\n",
    "#     hsv_v=0.2             # Brightness/contrast jitter\n",
    "# )\n",
    "\n",
    "result = model.train(\n",
    "    data='bdd100k.yaml',\n",
    "    epochs=1,\n",
    "    imgsz=256,  \n",
    "    batch=2,    \n",
    "    device='mps',\n",
    "    workers=2,\n",
    "    cache='ram' \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9c6c34",
   "metadata": {},
   "source": [
    "# <b>4. EVALUATION</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b271a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance on the validation set\n",
    "metrics = model.val()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
